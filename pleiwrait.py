import asyncio
import os
import json
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from langchain_google_genai import ChatGoogleGenerativeAI

load_dotenv()

# Check for Google API Key
google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise EnvironmentError("GOOGLE_API_KEY environment variable is not set. Please set it in your .env file or environment.")

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

SELECTOR = "body"  # Change to your target CSS selector

async def scrape_and_ask():
    # Ask LLM for a random, real website URL
    prompt_url = "Berikan satu URL website nyata, relevan, dan benar-benar ada yang berisi informasi tentang 'Karina aespa' (idol K-pop), hanya URL-nya saja, tanpa penjelasan apapun. REPOND DENGAN BAHASA INDONESIA"
    # llm.invoke returns an AIMessage, so access .content
    url_msg = llm.invoke(prompt_url)
    import re
    search_query = "Karina aespa"
    url_raw = url_msg.content.strip()
    # Try to extract a valid URL from Markdown or plain text
    match = re.search(r'https?://[^\s\]\)]+', url_raw)
    if match:
        url = match.group(0)
    else:
        # Try Markdown [text](url)
        match_md = re.search(r'\((https?://[^\s\)]+)\)', url_raw)
        if match_md:
            url = match_md.group(1)
        else:
            # If only domain, prepend https://
            url = url_raw.split()[0]
            if not url.startswith('http'):
                url = 'https://' + url.lstrip('[').rstrip(']()/')
    print(f"Generated by LLM, scraping: {url}")
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        # Try Google Search first
        try:
            await page.goto(f"https://www.google.com/search?q={search_query.replace(' ', '+')}", timeout=20000)
            await page.wait_for_selector('a h3', timeout=10000)
            # Find the first search result link (skip ads and special boxes)
            links = await page.query_selector_all('a h3')
            url = None
            for h3 in links:
                parent = await h3.evaluate_handle('node => node.parentElement')
                href = await parent.get_attribute('href')
                if href and href.startswith('http'):
                    url = href
                    break
            if not url:
                # Fallback: extract from Google's redirect URLs
                anchors = await page.query_selector_all('a')
                for a in anchors:
                    href = await a.get_attribute('href')
                    if href and href.startswith('/url?q='):
                        url = href.split('/url?q=')[1].split('&')[0]
                        if url.startswith('http'):
                            break
            if not url:
                raise Exception('No valid search result found on Google.')
        except Exception as e:
            # Fallback: try DuckDuckGo
            try:
                await page.goto(f"https://duckduckgo.com/?q={search_query.replace(' ', '+')}", timeout=20000)
                try:
                    await page.wait_for_selector('a.result__a', timeout=7000)
                    anchors = await page.query_selector_all('a.result__a')
                except Exception:
                    # Try a more generic selector if 'a.result__a' fails
                    await page.wait_for_selector('a', timeout=7000)
                    anchors = await page.query_selector_all('a')
                url = None
                for a in anchors:
                    href = await a.get_attribute('href')
                    if href and href.startswith('http'):
                        url = href
                        break
                if not url:
                    print(json.dumps({"error": f"Failed to find a search result: {str(e)}"}, indent=2, ensure_ascii=False))
                    await browser.close()
                    return
            except Exception as e2:
                print(json.dumps({"error": f"DuckDuckGo search also failed: {str(e2)}"}, indent=2, ensure_ascii=False))
                await browser.close()
                return
        print(f"Found via search, scraping: {url}")
        try:
            await page.goto(url, timeout=15000)
            await page.wait_for_selector(SELECTOR, timeout=10000)
            content = await page.inner_text(SELECTOR)
        except Exception as e:
            print(json.dumps({"error": f"Failed to scrape {url}: {str(e)}"}, indent=2, ensure_ascii=False))
            await browser.close()
            return
        await browser.close()

    # Use LLM to process scraped content
    prompt = f"Ringkas (summarize) konten berikut:\n{content[:4000]}"  # Limit to 4k chars for LLM
    result_msg = llm.invoke(prompt)
    result = result_msg.content if hasattr(result_msg, 'content') else str(result_msg)
    try:
        print(json.dumps({"url": url, "result": result}, indent=2, ensure_ascii=False))
    except Exception:
        print(result)
        try:
            await page.goto(url, timeout=15000)
            await page.wait_for_selector(SELECTOR, timeout=10000)
            content = await page.inner_text(SELECTOR)
        except Exception as e:
            print(json.dumps({"error": f"Failed to scrape {url}: {str(e)}"}, indent=2, ensure_ascii=False))
            await browser.close()
            return
        await browser.close()

    # Use LLM to process scraped content
    prompt = f"Ringkas (summarize) konten berikut:\n{content[:4000]}"  # Limit to 4k chars for LLM
    result_msg = llm.invoke(prompt)
    result = result_msg.content if hasattr(result_msg, 'content') else str(result_msg)
    try:
        print(json.dumps({"url": url, "result": result}, indent=2, ensure_ascii=False))
    except Exception:
        print(result)

if __name__ == "__main__":
    asyncio.run(scrape_and_ask())
